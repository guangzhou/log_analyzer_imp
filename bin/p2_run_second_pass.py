# -*- coding: utf-8 -*-
"""
ç¬¬äºŒé P2: æŒ‰å…³é”®ç»´åº¦ç»Ÿè®¡ä¸æ—¶é—´åˆ†å¸ƒ
æ–°å¢ï¼šç»Ÿè®¡æ•´ä¸ªæ–‡ä»¶çš„åŒ¹é…æ€»æ•° matched_linesï¼Œå¹¶å†™å…¥ run_session.matched_lines
"""
import os, argparse
from store import dao
from core import reader, parser as parser_mod, matcher, indexer as indexer_mod
from core.utils.config import load_yaml

def _derive_normal_path(path: str, override: str = None) -> str:
    if override:
        return override
    base = path[:-3] if path.endswith(".gz") else os.path.splitext(path)[0]
    return base + ".normal.txt"

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--path", required=True, help="gz æ—¥å¿—æ–‡ä»¶è·¯å¾„")
    ap.add_argument("--normal-in", default=None, help="å·²ç”Ÿæˆçš„ normal æ–‡ä»¶è·¯å¾„ï¼›è‹¥ä¸ä¼ åˆ™æŒ‰çº¦å®šæ¨å¯¼")
    ap.add_argument("--chunk-lines", type=int, default=None)
    ap.add_argument("--micro-batch", type=int, default=None)
    ap.add_argument("--match-workers", type=int, default=None)
    
    # ğŸ‘‡ æ·»åŠ è¿™ä¸€è¡Œï¼
    ap.add_argument("--file-id", type=str, default="auto", help="æ–‡ä»¶æ ‡è¯†ç¬¦ï¼Œç”¨äºè¾“å‡ºå‘½åç­‰")
    ap.add_argument("--config", type=str, default="configs/application.yaml")
    args = ap.parse_args()

    appcfg = load_yaml(args.config) or {}
    sp = appcfg.get("second_pass", {})
    chunk_lines = args.chunk_lines or sp.get("read_chunk_lines", 10000)
    micro_batch = args.micro_batch or sp.get("micro_batch_size", 200)
    match_workers = args.match_workers or sp.get("match_workers_per_batch", 4)

    path = args.path
    normal_path = args.normal_in or _derive_normal_path(path)

    # ä¼šè¯
    # ä¸ P1 ä¿æŒä¸€è‡´ï¼šå¦‚æœé¡¹ç›®å·²æœ‰ calc_file_idï¼Œè¯·æ›¿æ¢ä¸ºç›¸åŒå®ç°
    file_id = os.path.basename(path)
    run_id = dao.create_run_session(file_id, "ç¬¬äºŒé", dict(chunk_lines=chunk_lines, micro_batch=micro_batch))

    # ç´¢å¼•
    idx = indexer_mod.Indexer()
    idx.load_initial()

    file_matched_total = 0
    buffer = []

    for chunk in reader.read_in_chunks(normal_path, chunk_lines=chunk_lines):
        for line in chunk:
            p = parser_mod.parse_fields(line)
            if p:
                buffer.append(p)
                if len(buffer) >= micro_batch:
                    results = matcher.match_batch(idx.get_active(), buffer, workers=match_workers)
                    file_matched_total += sum(1 for r in results if getattr(r, "is_hit", False))
                    # TODO: ä¿æŒåŸæœ‰èšåˆä¸å†™åº“é€»è¾‘ï¼ˆæ­¤å¤„ä¸æ”¹åŠ¨ï¼Œåªæ–°å¢æ€»æ•°è®¡æ•°ï¼‰
                    buffer.clear()

    if buffer:
        results = matcher.match_batch(idx.get_active(), buffer, workers=match_workers)
        file_matched_total += sum(1 for r in results if getattr(r, "is_hit", False))
        buffer.clear()

    dao.complete_run_session(run_id, matched_lines=file_matched_total, status="æˆåŠŸ")
    print(f"[OK] ç¬¬äºŒéå®Œæˆ file_id={file_id}, matched_lines={file_matched_total}")

if __name__ == "__main__":
    main()